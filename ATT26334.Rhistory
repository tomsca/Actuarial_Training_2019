?glmnet
?glmnet
?glmnet
?glmnet
??glmnet
library(glmnet)
?glmnet
data(MultiGaussianExample)
head(MultiGaussianExample)
load("MultiGaussianExample.RData")
data(QuickStartExample)
head(MultiGaussianExample)
install.packages("beta_CVX")
install.packages("MultiGaussianExample")
install.packages("datasets.load")
head(MultiGaussianExample)
load("MultiGaussianExample.RData")
load("C:/Users/ahmad/Desktop/MultiGaussianExample.RData")
head(MultiGaussianExample)
load("MultiGaussianExample.RData")
dat=load("C:/Users/ahmad/Desktop/MultiGaussianExample.RData")
dat
dat=load("C:/Users/ahmad/Desktop/MultiGaussianExample.RData")
x
y
data(canlifins)
head(canlifins)
library(CASdatasets)
data(canlifins)
head(canlifins)
cor(canlifins$EntryAgeM,canlifins$EntryAgeF)
library(CASdatasets)
data(canlifins)
head(canlifins)
canlifins_data <- canlifins
head(canlifins_data)
canlifins_data=na.omit(canlifins_data)
x=model.matrix(c(EntryAgeM,EntryAgeF)~.,canlifins_data)
canlifins_data$DeathTimeF
c(EntryAgeM,EntryAgeF)
canlifins_data
head(canlifins_data)
c(EntryAgeM,EntryAgeF)
x=model.matrix(cbind(EntryAgeM,EntryAgeF)~.,canlifins_data)
y=canlifins_data[]
y=canlifins_data[-c(3,4,5)]
## set the seed to make your partition reproducible
##80% of the sample size
set.seed(123)
sample_size <- floor(0.80 * nrow(canlifins_data))
train=sample(1:nrow(canlifins_data),sample_size,replace=FALSE)
#Find the optimal lambda value via cross validation
cv.out=cv.glmnet(x[train,],y[train],alpha=0, nfolds = 10, family="gaussian", type.measure="mse", standardize=TRUE)
train=sample(1:nrow(canlifins_data),sample_size,replace=FALSE)
train
x=model.matrix(cbind(EntryAgeM,EntryAgeF)~.,canlifins_data);x
x=model.matrix(cbind(EntryAgeM,EntryAgeF)~.,canlifins_data)[-1]
y=canlifins_data[-c(3,4,5)]
## set the seed to make your partition reproducible
##80% of the sample size
set.seed(123)
sample_size <- floor(0.80 * nrow(canlifins_data))
train=sample(1:nrow(canlifins_data),sample_size,replace=FALSE)
#Find the optimal lambda value via cross validation
cv.out=cv.glmnet(x[train,],y[train],alpha=0, nfolds = 10, family="gaussian", type.measure="mse", standardize=TRUE)
x=model.matrix(cbind(EntryAgeM,EntryAgeF)~.,canlifins_data)[-1]
x=model.matrix(cbind(EntryAgeM,EntryAgeF)~.,canlifins_data)[-1];x
x=model.matrix(cbind(EntryAgeM,EntryAgeF)~.,canlifins_data)[,-1]
x=model.matrix(cbind(EntryAgeM,EntryAgeF)~.,canlifins_data)[,-1];x
y=canlifins_data[-c(3,4,5)]
## set the seed to make your partition reproducible
##80% of the sample size
set.seed(123)
sample_size <- floor(0.80 * nrow(canlifins_data))
train=sample(1:nrow(canlifins_data),sample_size,replace=FALSE)
#Find the optimal lambda value via cross validation
cv.out=cv.glmnet(x[train,],y[train],alpha=0, nfolds = 10, family="gaussian", type.measure="mse", standardize=TRUE)
bestlam=cv.out$lambda.min
x[train,]
#Find the optimal lambda value via cross validation
cv.out=cv.glmnet(x[train,],y[train],alpha=0, nfolds = 10, family="mgaussian", type.measure="mse", standardize=TRUE)
#Find the optimal lambda value via cross validation
cv.out=cv.glmnet(x[train,],y[train],alpha=0, nfolds = 10, family="mgaussian", type.measure="mse", standardize=TRUE)
#Find the optimal lambda value via cross validation
library(glmnet)
cv.out=cv.glmnet(x[train,],y[train],alpha=0, nfolds = 10, family="mgaussian", type.measure="mse", standardize=TRUE)
cv.out=cv.glmnet(x[c(train),],y[c(train)],alpha=0, nfolds = 10, family="mgaussian", type.measure="mse", standardize=TRUE)
bestlam=cv.out$lambda.min
cv.out=cv.glmnet(x[train,],y[train,],alpha=0, nfolds = 10, family="mgaussian", type.measure="mse", standardize=TRUE)
cv.out=cv.glmnet(x[train,],y[cbind(train),],alpha=0, nfolds = 10, family="mgaussian", type.measure="mse", standardize=TRUE)
y[train,]
x[train,]
cv.out=cv.glmnet(x[train,],y[train,],alpha=0, nfolds = 10, family="mgaussian", type.measure="mse")
#Find the optimal lambda value via cross validation
library(glmnet)
cv.out=cv.glmnet(x[train,],y[train,],alpha=0, nfolds = 10, family="mgaussian", type.measure="mse")
cv.out=cv.glmnet(x[train,],y[train,],alpha=0, nfolds = 10, family="mgaussian", type.measure="mse")
#Find the optimal lambda value via cross validation
library(glmnet)
cv.out=cv.glmnet(x,y,alpha=0, nfolds = 10, family="mgaussian", type.measure="mse")
y=canlifins_data[-c(3,4,5)];y
data(canlifins)
head(canlifins)
canlifins_data <- canlifins
head(canlifins_data)
canlifins_data$DeathTimeF
canlifins_data=na.omit(canlifins_data)
cbind(EntryAgeM,EntryAgeF)
x=model.matrix(cbind(EntryAgeM,EntryAgeF)~.,canlifins_data)[,-1];x
y=canlifins_data[-c(3,4,5)];y
#Find the optimal lambda value via cross validation
library(glmnet)
cv.out=cv.glmnet(x,y,alpha=0, nfolds = 10, family="mgaussian", type.measure="mse")
cv.out=cv.glmnet(x,y,alpha=0, nfolds = 10, family="mgaussian")
cv.out=cv.glmnet(x,y,alpha=0, nfolds = 10, family="mgaussian", type.measure="mse")
head(canlifins_data)
x=canlifins_data[-c(1,2)];x
y=canlifins_data[-c(3,4,5)];y
#Find the optimal lambda value via cross validation
library(glmnet)
cv.out=cv.glmnet(x,y,alpha=0, nfolds = 10, family="mgaussian", type.measure="mse")
x=as.matrix(canlifins_data[-c(1,2)]);x
y=as.matrix(canlifins_data[-c(3,4,5)]);y
#Find the optimal lambda value via cross validation
library(glmnet)
cv.out=cv.glmnet(x,y,alpha=0, nfolds = 10, family="mgaussian", type.measure="mse")
bestlam=cv.out$lambda.min
plot(cv.out)
#Fit a ridge regression model
ridge.mod=glmnet(x,y,alpha = 0, lambda=bestlam, family="mgaussian")
#Compute the test error w/ lambda chosen by cross validation
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[-train,])
ridge.mse=round(mean((ridge.pred-y[-train])^2))
#Store ridge coefficients
ridge.coef=predict(ridge.mod,type="coefficients",s=bestlam)
ridge.coef
data(canlifins)
head(canlifins)
canlifins_data <- canlifins
head(canlifins_data)
canlifins_data$DeathTimeF
canlifins_data=na.omit(canlifins_data)
x=as.matrix(canlifins_data[-c(1,2)]);x
y=as.matrix(canlifins_data[-c(3,4,5)]);y
#Find the optimal lambda value via cross validation
library(glmnet)
cv.out=cv.glmnet(x,y,alpha=0, nfolds = 10, family="mgaussian", type.measure="mse")
bestlam=cv.out$lambda.min
plot(cv.out)
#Store ridge coefficients
ridge.coef=predict(ridge.mod, type.coef="2norm",s=bestlam)
ridge.coef
#Store ridge coefficients
ridge.coef=predict(ridge.mod, type.coef="2norm",s=bestlam)
#Fit a ridge regression model
ridge.mod=glmnet(x,y,alpha = 0, lambda=bestlam, family="mgaussian")
#Compute the test error w/ lambda chosen by cross validation
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[-train,])
ridge.mse=round(mean((ridge.pred-y[-train])^2))
#Store ridge coefficients
ridge.coef=predict(ridge.mod, type.coef="2norm",s=bestlam)
data(canlifins)
head(canlifins)
canlifins_data <- canlifins
head(canlifins_data)
canlifins_data$DeathTimeF
canlifins_data=na.omit(canlifins_data)
x=as.matrix(canlifins_data[-c(1,2)]);x
y=as.matrix(canlifins_data[-c(3,4,5)]);y
#Find the optimal lambda value via cross validation
library(glmnet)
cv.out=cv.glmnet(x,y,alpha=0, nfolds = 10, family="mgaussian", type.measure="mse")
bestlam=cv.out$lambda.min
bestlam
plot(cv.out)
#Fit a ridge regression model
ridge.mod=glmnet(x,y,alpha = 0, lambda=bestlam, family="mgaussian")
#Compute the test error w/ lambda chosen by cross validation
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[-train,])
ridge.mse=round(mean((ridge.pred-y[-train])^2))
x{[1:5]}
x[1:5]
x[1:5,]
x[1:2,]
bestlam=cv.out$lambda.min
plot(cv.out)
#Fit a ridge regression model
ridge.mod=glmnet(x,y,alpha = 0, lambda=bestlam, family="mgaussian")
#Compute the test error w/ lambda chosen by cross validation
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[-train,])
#Compute the test error w/ lambda chosen by cross validation
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[-train,])
#Store ridge coefficients
ridge.coef=predict(ridge.mod, type="coefficients",s=bestlam)
ridge.coef
#Find the optimal lambda value via cross validation
cv.out=cv.glmnet(x,y,alpha=1, nfolds = 10, family="mgaussian")
bestlam=cv.out$lambda.min
plot(cv.out)
#Train the model
lasso.mod=glmnet(x,y,alpha = 1, lambda=bestlam, family="gaussian")
#Train the model
lasso.mod=glmnet(x,y,alpha = 1, lambda=bestlam, family="mgaussian")
#Compute the test error
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[-train,])
lasso.mse=round(mean((lasso.pred-y[-train])^2))
#Store lasso coefficients
lasso.coef=predict(lasso.mod,type="coefficients",s=bestlam)
lasso.coef
#Find the optimal lambda value via cross validation
cv.out=cv.glmnet(x[train,],y[train],alpha=0.3, nfolds = 10, family="gaussian",type.measure="mse", standardize=TRUE)
#Find the optimal lambda value via cross validation
cv.out=cv.glmnet(x,y,alpha=0.3, nfolds = 10, family="mgaussian")
bestlam=cv.out$lambda.min
plot(cv.out)
#Train the model
enet.mod=glmnet(x,y,alpha = 0.3, lambda=bestlam, family="gaussian")
#Train the model
enet.mod=glmnet(x,y,alpha = 0.3, lambda=bestlam, family="mgaussian")
#Compute the test error
enet.pred=predict(enet.mod,s=bestlam,newx=x[-train,])
#Store elastic net coefficients
enet.coef=predict(enet.mod,type="coefficients",s=bestlam)
enet.coef
coefs=cbind(as.vector(ridge.coef),as.vector(lasso.coef),as.vector(enet.coef))
colnames(coefs)=c("ridge","lasso", "enet")
mse <- c(ridge.mse, lasso.mse, enet.mse)
#Compute the test error
enet.pred=predict(enet.mod,s=bestlam,newx=x[-train,])
#Compute the test error
enet.pred=predict(enet.mod,s=bestlam,newx=x[-1:5,])
#Store elastic net coefficients
enet.coef=predict(enet.mod,type="coefficients",s=bestlam)
#Compute the test error
enet.pred=predict(enet.mod,s=bestlam,newx=x[-c(1:5),])
#Store elastic net coefficients
enet.coef=predict(enet.mod,type="coefficients",s=bestlam)
enet.coef
coefs=cbind(as.vector(ridge.coef),as.vector(lasso.coef),as.vector(enet.coef))
library(CASdatasets)
data(canlifins)
data(canlifins)
head(canlifins)
head(canlifins_data)
canlifins_data$DeathTimeF
canlifins_data$DeathTimeF
canlifins_data=na.omit(canlifins_data)
canlifins_data=na.omit(canlifins_data)
x=as.matrix(canlifins_data[-c(1,2)]);x
y=as.matrix(canlifins_data[-c(3,4,5)]);y
#Find the optimal lambda value via cross validation
library(glmnet)
x=as.matrix(canlifins_data[-c(1,2)]);x
y=as.matrix(canlifins_data[-c(3,4,5)]);y
#Find the optimal lambda value via cross validation
library(glmnet)
cv.out=cv.glmnet(x,y,alpha=0, nfolds = 10, family="mgaussian")
canlifins_data$DeathTimeF
canlifins_data=na.omit(canlifins_data)
x=as.matrix(canlifins_data[-c(1,2)]);x
y=as.matrix(canlifins_data[-c(3,4,5)]);y
#Find the optimal lambda value via cross validation
library(glmnet)
cv.out=cv.glmnet(x,y,alpha=0, nfolds = 10, family="mgaussian")
bestlam=cv.out$lambda.min
plot(cv.out)
#Fit a ridge regression model
ridge.mod=glmnet(x,y,alpha = 0, lambda=bestlam, family="mgaussian")
#Compute the test error w/ lambda chosen by cross validation
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[-train,])
#Compute the test error w/ lambda chosen by cross validation
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[1:50,])
#Store ridge coefficients
ridge.coef=predict(ridge.mod, type="coefficients",s=bestlam)
ridge.coef
#Find the optimal lambda value via cross validation
cv.out=cv.glmnet(x,y,alpha=1, nfolds = 10, family="mgaussian")
bestlam=cv.out$lambda.min
plot(cv.out)
#Train the model
lasso.mod=glmnet(x,y,alpha = 1, lambda=bestlam, family="mgaussian")
#Compute the test error
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[1:50,])
#Store lasso coefficients
lasso.coef=predict(lasso.mod,type="coefficients",s=bestlam)
lasso.coef
#Find the optimal lambda value via cross validation
cv.out=cv.glmnet(x,y,alpha=0.3, nfolds = 10, family="mgaussian")
#Find the optimal lambda value via cross validation
cv.out=cv.glmnet(x,y,alpha=0.3, nfolds = 10, family="mgaussian")
bestlam=cv.out$lambda.min
bestlam=cv.out$lambda.min
plot(cv.out)
#Train the model
enet.mod=glmnet(x,y,alpha = 0.3, lambda=bestlam, family="mgaussian")
#Compute the test error
enet.pred=predict(enet.mod,s=bestlam,newx=x[1:50,])
#Store elastic net coefficients
enet.coef=predict(enet.mod,type="coefficients",s=bestlam)
enet.coef
coefs=cbind(as.vector(ridge.coef),as.vector(lasso.coef),as.vector(enet.coef))
colnames(coefs)=c("ridge","lasso", "enet")
rbind(coefs,mse)
coefs
coefs=cbind(as.vector(ridge.coef),as.vector(lasso.coef),as.vector(enet.coef))
colnames(coefs)=c("ridge","lasso", "enet")
coefs
enet.coef
enet.coef
head(canlifins)
head(canlifins)
library(CASdatasets)
data(usexpense)
usexpense_data <- usexpense[-c(1,2,3,4)]
head(usexpense_data)
usexpense_data=na.omit(usexpense_data)
x=model.matrix(RBC~.,usexpense_data)[,-1]
y=usexpense_data$RBC
## 80% of the sample size
## set the seed to make your partition reproducible
set.seed(123)
sample_size <- floor(0.80 * nrow(usexpense_data))
cv.out=cv.glmnet(x[train,],y[train],alpha=0, nfolds = 10, family="gaussian", type.measure="mse", standardize=TRUE)
## 80% of the sample size
## set the seed to make your partition reproducible
set.seed(123)
sample_size <- floor(0.80 * nrow(usexpense_data))
train=sample(1:nrow(usexpense_data),sample_size,replace=FALSE)
#Find the optimal lambda value via cross validation
cv.out=cv.glmnet(x[train,],y[train],alpha=0, nfolds = 10, family="gaussian", type.measure="mse", standardize=TRUE)
bestlam=cv.out$lambda.min
plot(cv.out)
#Fit a ridge regression model
ridge.mod=glmnet(x,y,alpha = 0, lambda=bestlam, family="gaussian")
#Compute the test error w/ lambda chosen by cross validation
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[-train,])
ridge.mse=round(mean((ridge.pred-y[-train])^2))
#Store ridge coefficients
ridge.coef=predict(ridge.mod,type="coefficients",s=bestlam)
ridge.coef
cv.out=cv.glmnet(x[train,],y[train],alpha=1, nfolds = 10, family="gaussian",type.measure="mse", standardize=TRUE)
bestlam=cv.out$lambda.min
plot(cv.out)
#Train the model
lasso.mod=glmnet(x,y,alpha = 1, lambda=bestlam, family="gaussian")
#Compute the test error
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[-train,])
lasso.mse=round(mean((lasso.pred-y[-train])^2))
#Store lasso coefficients
lasso.coef=predict(lasso.mod,type="coefficients",s=bestlam)
lasso.coef
#Find the optimal lambda value via cross validation
cv.out=cv.glmnet(x[train,],y[train],alpha=0.3, nfolds = 10, family="gaussian",type.measure="mse", standardize=TRUE)
bestlam=cv.out$lambda.min
plot(cv.out)
#Train the model
enet.mod=glmnet(x,y,alpha = 0.3, lambda=bestlam, family="gaussian")
#Compute the test error
enet.pred=predict(enet.mod,s=bestlam,newx=x[-train,])
enet.mse=round(mean((enet.pred-y[-train])^2))
#Store elastic net coefficients
enet.coef=predict(enet.mod,type="coefficients",s=bestlam)
enet.coef
coefs=cbind(as.vector(ridge.coef),as.vector(lasso.coef),as.vector(enet.coef))
colnames(coefs)=c("ridge","lasso", "enet")
mse <- c(ridge.mse, lasso.mse, enet.mse)
rbind(coefs,mse)
library(CASdatasets)
data(hurricanehist)
head(hurricanehist)
hurricanehist_data= hurricanehist[-1]
hurricanehist_data=na.omit(hurricanehist_data)
x=model.matrix(Region ~.,hurricanehist_data)[,-1];
y=as.factor(hurricanehist_data$Region)
set.seed(123)
sample_size <- floor(0.50 * nrow(hurricanehist_data))
train=sample(1:nrow(hurricanehist_data),sample_size,replace=FALSE)
cv.out=cv.glmnet(x[train,],y[train],alpha=0, nfolds = 10, family="multinomial", type.measure="class", standardize=TRUE)
bestlam=cv.out$lambda.min
#Fit a ridge regression model
ridge.mod=glmnet(x,y,alpha = 0, lambda=bestlam, family="multinomial")
#Compute the test error w/ lambda chosen by cross validation
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[-train,])
#Store ridge coefficients
ridge.coef=predict(ridge.mod,type="coefficients",s=bestlam)
ridge.coef
#Find the optimal lambda value via cross validation
cv.out=cv.glmnet(x[train,],y[train],alpha=1, nfolds = 10, family="multinomial",type.measure="class", standardize=TRUE)
bestlam=cv.out$lambda.min
#Train the model
lasso.mod=glmnet(x,y,alpha = 1, lambda=bestlam, family="multinomial")
#Compute the test error
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[-train,])
#Store lasso coefficients
lasso.coef=predict(lasso.mod,type="coefficients",s=bestlam)
lasso.coef
#Find the optimal lambda value via cross validation
cv.out=cv.glmnet(x[train,],y[train],alpha=0.3, nfolds = 10, family="multinomial",type.measure="class", standardize=TRUE)
bestlam=cv.out$lambda.min
#Train the model
enet.mod=glmnet(x,y,alpha = 0.3, lambda=bestlam, family="multinomial")
#Compute the test error
enet.pred=predict(enet.mod,s=bestlam,newx=x[-train,])
#Store lasso coefficients
enet.coef=predict(enet.mod,type="coefficients",s=bestlam)
enet.coef
Consider the data **fremotor1prem_data** in which we take the total premium **PremTot** as the response variable. A description of the data can be found in this link  http://cas.uqam.ca/pub/R/web/CASdatasets-manual.pdf
Here is the set up of the data:
library(CASdatasets)
data(fremotor1prem)
head(fremotor1prem)
fremotor1prem_data <- fremotor1prem[-c(1,31)]
fremotor1prem_data=na.omit(fremotor1prem_data)
x=model.matrix(PremTot ~.,fremotor1prem_data)[,-1]
y=fremotor1prem_data$PremTot
set.seed(123)
sample_size <- floor(0.70 * nrow(fremotor1prem_data))
train=sample(1:nrow(fremotor1prem_data),sample_size,replace=FALSE)
for (i in seq(0, 1, .1)){
enet.mse <- data.frame()
#Using cross-validation to choose the tuning parameter ??
set.seed (123)
cv.out=cv.glmnet(x[train,],y[train],alpha=i, nfolds = 10,
family="gaussian",type.measure="mse", standardize=TRUE)
bestlam=cv.out$lambda.min
#Creating training model using lasso regression
enet.mod=glmnet(x,y,alpha = i, lambda=bestlam, family="gaussian")
#Printing out
#Compute the test error
enet.pred=predict(enet.mod,s=bestlam,newx=x[-train,])
enet.mse=mean((enet.pred-y[-train])^2)
print(c(enet.mse,i))
}
library(glmnet)
wholesale_data <- read.csv("wholesale_customers.csv", header=TRUE, sep=",")
wholesale_data=na.omit(wholesale_data)
x=as.matrix(eqlist_data[-4])
library(CASdatasets)
data(eqlist)
head(eqlist)
eqlist_data <- eqlist[-c(1,6,7,11:16)]
head(eqlist_data)
eqlist_data=na.omit(eqlist_data)
x=as.matrix(eqlist_data[-4])
y=as.vector(eqlist_data$mag)
fitRidge <- function(X, y, lambda){
# Add intercept column to X:
X <- cbind(1, X)
# Calculate penalty matrix:
lambda.diag <- lambda * diag(dim(X)[2])
# Apply formula for Ridge Regression:
return(solve(t(X) %*% X + lambda.diag)%*%t(X) %*% y)
}
fitRidge(x,y,0.10)
library(CASdatasets)
data(swmotorcycle)
head(swmotorcycle)
swmotorcycle_data <- swmotorcycle[-c(3,4)]
head(swmotorcycle_data)
lasso.mod=glmnet(x,y,alpha = 1, lambda=bestlam_lambda.min, family="gaussian")
swmotorcycle_data=na.omit(swmotorcycle_data)
x=model.matrix(ClaimAmount ~.,swmotorcycle_data)[,-1]
y=swmotorcycle_data$ClaimAmount
## set the seed to make your partition reproducible
##80% of the sample size
set.seed(123)
sample_size <- floor(0.70 * nrow(swmotorcycle_data))
train=sample(1:nrow(swmotorcycle_data),sample_size,replace=FALSE)
#Find the optimal lambda value via cross validation using
cv.out=cv.glmnet(x[train,],y[train],alpha=1, nfolds = 5, family="gaussian",type.measure="mse", standardize=TRUE)
bestlam_lambda.min=cv.out$lambda.min
plot(cv.out)
#Train the model
lasso.mod=glmnet(x,y,alpha = 1, lambda=bestlam_lambda.min, family="gaussian")
#Compute the test error
lasso.pred=predict(lasso.mod,s=bestlam_lambda.min,newx=x[-train,])
lasso.mse.lambda.min=round(mean((lasso.pred-y[-train])^2))
#Store lasso coefficients
lasso.coef.lambda.min=predict(lasso.mod,type="coefficients",s=bestlam_lambda.min)
lasso.coef.lambda.min=round(lasso.coef.lambda.min,6)
#Find the optimal lambda value via cross validation using
cv.out=cv.glmnet(x[train,],y[train],alpha=1, nfolds = 5, family="gaussian",type.measure="mse", standardize=TRUE)
bestlam_lambda.1se=cv.out$lambda.1se
plot(cv.out)
#Train the model
lasso.mod=glmnet(x,y,alpha = 1, lambda=bestlam_lambda.1se, family="gaussian")
#Compute the test error
lasso.pred=predict(lasso.mod,s=bestlam_lambda.1se,newx=x[-train,])
lasso.mse.lambda.1se=round(mean((lasso.pred-y[-train])^2))
#Store lasso coefficients
lasso.coef.lambda.1se=predict(lasso.mod,type="coefficients",s=bestlam_lambda.1se)
lasso.coef.lambda.1se=round(lasso.coef.lambda.1se,6)
coefs=cbind(as.vector(lasso.coef.lambda.min),as.vector(lasso.coef.lambda.1se))
colnames(coefs)=c("lasso.min.lambda", "lasso.1st.lambda")
mse <- c(lasso.mse.lambda.min, lasso.mse.lambda.1se)
rbind(coefs,mse)
